{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a9f0152",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Statistical Types for üêº Pandas DataFrames and Friends üåà‚ú®\n",
    "\n",
    "### Parse, Validate, and Synthesize DataFrames with Generative Schemas.\n",
    "\n",
    "**Niels Bantilan**, Chief ML Engineer @ Union.ai\n",
    "\n",
    "*Python Live Webinars - Amicus, July 28th 2022*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b47e5391",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide_input",
     "hide_output"
    ]
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandera'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a642e5edfc50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandera\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandera'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import pandera as pa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4195b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Background üèû\n",
    "\n",
    "- üìú B.A. in Biology and Dance\n",
    "- üìú M.P.H. in Sociomedical Science and Public Health Informatics\n",
    "- ü§ñ Chief Machine Learning Engineer @ Union.ai\n",
    "- üõ© Flytekit OSS Maintainer\n",
    "- ‚úÖ Author and Maintainer of Pandera\n",
    "- ü¶æ Author of UnionML\n",
    "- üõ† Make DS/ML practitioners more productive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863d4960",
   "metadata": {
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outline üìù\n",
    "\n",
    "- ü§∑‚Äç‚ôÇÔ∏è Why Should I Validate Data?\n",
    "- ü§î What's Data Testing, and How Can I Put it Into Practice?\n",
    "- ‚úÖ Pandera Quickstart: create statistical types for your DataFrames\n",
    "- üìä Example 1: Validate your Data analysis\n",
    "- ü§ñ Example 2: Validate your Machine Learning Pipeline\n",
    "- ‚≠êÔ∏è Conclusion: How can I start using Pandera in my work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e229e7cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ü§∑‚Äç‚ôÇÔ∏è Why Should I Validate Data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8681598",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37c9a50",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What's a `DataFrame`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68a48cc",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "dataframe = pd.DataFrame({\n",
    "    \"person_id\": [str(uuid.uuid4())[:7] for _ in range(6)],\n",
    "    \"hours_worked\": [38.5, 41.25, \"35.0\", 27.75, 22.25, -20.5],\n",
    "    \"wage_per_hour\": [15.1, 15, 21.30, 17.5, 19.50, 25.50],\n",
    "}).set_index(\"person_id\")\n",
    "\n",
    "df = dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28bbd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d4b54e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What's Data Validation?\n",
    "\n",
    "Data validation is the act of _falsifying_ data against explicit assumptions\n",
    "for some downstream purpose, like analysis, modeling, and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457b4e3c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> \"All swans are white\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90657d6d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<p>\n",
    "    <a href=\"https://commons.wikimedia.org/wiki/File:Black_Swans.jpg#/media/File:Black_Swans.jpg\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/6/60/Black_Swans.jpg\" alt=\"Pair of black swans swimming\" height=\"480\" width=\"275\"\n",
    "     style=\"display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "    </a>\n",
    "    <p style=\"font-size: x-small; text-align: center;\">\n",
    "    <a href=\"http://creativecommons.org/licenses/by-sa/3.0/\" title=\"Creative Commons Attribution-Share Alike 3.0\">CC BY-SA 3.0</a>,\n",
    "    <a href=\"https://commons.wikimedia.org/w/index.php?curid=1243220\">Link</a>\n",
    "    </p>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ee5ec1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why Do I Need it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cbef2e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### üêû It can be difficult to reason about and debug data processing pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e268354e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### ‚ö†Ô∏è It's critical to ensuring data quality in many contexts especially when the end product informs business decisions, supports scientific findings, or generates predictions in a production setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63ca74b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Everyone has a personal relationship with their dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7795c433",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Story Time üìñ\n",
    "\n",
    "##### Imagine that you're a data scientist maintaining an existing data processing pipeline üë©‚Äçüíªüë®‚Äçüíª..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c437e5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def process_data(df):\n",
    "    return df.assign(weekly_income=lambda x: x.hours_worked * x.wage_per_hour)\n",
    "\n",
    "try:\n",
    "    process_data(dataframe)\n",
    "except TypeError as exc:\n",
    "    print(exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c6d38a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One day, you encounter an error log trail and decide to follow it..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5167c0a2",
   "metadata": {},
   "source": [
    "```python\n",
    "/usr/local/miniconda3/envs/pandera-presentations/lib/python3.7/site-packages/pandas/core/ops/__init__.py in masked_arith_op(x, y, op)\n",
    "    445         if mask.any():\n",
    "    446             with np.errstate(all=\"ignore\"):\n",
    "--> 447                 result[mask] = op(xrav[mask], com.values_from_object(yrav[mask]))\n",
    "    448 \n",
    "    449     else:\n",
    "\n",
    "TypeError: can't multiply sequence by non-int of type 'float'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04597d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### And you find yourself at the top of a function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1087d8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def process_data(df):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d3ad51",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### You look around, and see some hints of what had happened..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e81139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df):\n",
    "    return df.assign(weekly_income=lambda x: x.hours_worked * x.wage_per_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175cf461",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### You sort of know what's going on, but you want to take a closer look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00629a23",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def process_data(df):\n",
    "    import pdb; pdb.set_trace()  # <- insert breakpoint\n",
    "    return df.assign(weekly_income=lambda x: x.hours_worked * x.wage_per_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc80b10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### And you find some funny business going on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0182474",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dead523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e87613",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hours_worked.map(type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f6f533",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### You squash the bug and add documentation for the next weary traveler who happens upon this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24810ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df):\n",
    "    return (\n",
    "        df\n",
    "        # make sure columns are floats\n",
    "        .astype({\"hours_worked\": float, \"wage_per_hour\": float})\n",
    "        # replace negative values with nans\n",
    "        .assign(hours_worked=lambda x: x.hours_worked.where(x.hours_worked >= 0, np.nan))\n",
    "        # compute weekly income\n",
    "        .assign(weekly_income=lambda x: x.hours_worked * x.wage_per_hour)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c5923",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549db76e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ‚è± A few months later..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3750f03a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### You find yourself at a familiar function, but it looks a little different from when you left it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d1db91",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# This needs to be here, but skipped for story-telling effect in the slides\n",
    "import pandera as pa\n",
    "from pandera.typing import DataFrame, Series\n",
    "\n",
    "class RawData(pa.SchemaModel):\n",
    "    hours_worked: Series[float] = pa.Field(coerce=True, nullable=True)\n",
    "    wage_per_hour: Series[float] = pa.Field(coerce=True, nullable=True)\n",
    "\n",
    "class ProcessedData(RawData):\n",
    "    hours_worked: Series[float] = pa.Field(ge=0, coerce=True, nullable=True)\n",
    "    weekly_income: Series[float] = pa.Field(nullable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36824faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pa.check_types\n",
    "def process_data(df: DataFrame[RawData]) -> DataFrame[ProcessedData]:\n",
    "    return (\n",
    "        # replace negative values with nans\n",
    "        df.assign(hours_worked=lambda x: x.hours_worked.where(x.hours_worked >= 0, np.nan))\n",
    "        # compute weekly income\n",
    "        .assign(weekly_income=lambda x: x.hours_worked * x.wage_per_hour)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2301348",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### You look above and see what `RawData` and `ProcessedData` are, finding a `NOTE` that a fellow traveler has left for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a364f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandera as pa\n",
    "\n",
    "# NOTE: this is what's supposed to be in `df` going into `process_data`\n",
    "class RawData(pa.SchemaModel):\n",
    "    hours_worked: Series[float] = pa.Field(coerce=True, nullable=True)\n",
    "    wage_per_hour: Series[float] = pa.Field(coerce=True, nullable=True)\n",
    "\n",
    "\n",
    "# ... and this is what `process_data` is supposed to return.\n",
    "class ProcessedData(RawData):\n",
    "    hours_worked: Series[float] = pa.Field(ge=0, coerce=True, nullable=True)\n",
    "    weekly_income: Series[float] = pa.Field(nullable=True)\n",
    "\n",
    "\n",
    "@pa.check_types\n",
    "def process_data(df: DataFrame[RawData]) -> DataFrame[ProcessedData]:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8787755f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Moral of the Story"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997190c5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### The better you can reason about the contents of a dataframe, the faster you can debug."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8f8b5f",
   "metadata": {
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### The faster you can debug, the sooner you can focus on downstream tasks that you care about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a806523a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ü§î What's Data Testing\n",
    "\n",
    "### And How Can I Put it Into Practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1619eb03",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> **Data validation:** The act of falsifying data against explicit assumptions for some downstream purpose, like\n",
    "> analysis, modeling, and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9faf04",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> **Data Testing:** Validating not only real data, but also the functions that produce them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268a6be1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# In the Real World üåç\n",
    "\n",
    "#### Validate real data in production"
   ]
  },
  {
   "cell_type": "raw",
   "id": "49326d64",
   "metadata": {},
   "source": [
    "<div class=\"mermaid\">\n",
    "graph LR\n",
    "    R[(raw data)] --> RS([raw schema])\n",
    "    RS --> TF[transform function]\n",
    "    TF --> TS([transformed schema])\n",
    "    TS --> T[(transformed data)]\n",
    "\n",
    "    style RS fill:#8bedc6,stroke:#333\n",
    "    style TS fill:#8bedc6,stroke:#333\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b7ce07",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# In the Test Suite üß™\n",
    "\n",
    "#### Validate functions that produce data, given some test cases"
   ]
  },
  {
   "cell_type": "raw",
   "id": "17f8bdd9",
   "metadata": {},
   "source": [
    "<div class=\"mermaid\">\n",
    "graph LR\n",
    "    G([raw schema]) --> T1[(test case 1)]\n",
    "    G --> T2[(test case 2)]\n",
    "    G --> TN[(test case n)]\n",
    "\n",
    "    T1 --> TF[transform function]\n",
    "    T2 --> TF\n",
    "    TN --> TF\n",
    "    TF --> TS([transformed schema])\n",
    "    TS --> T[(transformed data)]\n",
    "\n",
    "    style G fill:#8bedc6,stroke:#333\n",
    "    style TS fill:#8bedc6,stroke:#333\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0fe81f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Testing in Practice\n",
    "\n",
    "#### Data testing is an iterative process of:\n",
    "\n",
    "- Building domain knowledge about the data at hand with respect to the stated goal.\n",
    "- Implementing data transforms and defining schemas/tests in parallel.\n",
    "- Verifying that the output of the data transform is what you expected."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e1979028",
   "metadata": {},
   "source": [
    "<div class=\"mermaid\">\n",
    "graph LR\n",
    "    \n",
    "    D[Define Goal]\n",
    "\n",
    "    subgraph Development Process\n",
    "        E[Explore Data]\n",
    "        S[Define Schema and Tests]\n",
    "        I[Implement Data Transform]\n",
    "        V[Verify Data]\n",
    "    end\n",
    "\n",
    "    P{Checks Pass?}\n",
    "    A[Continue Analysis...]\n",
    "\n",
    "    D --> E\n",
    "    E --> S\n",
    "    E --> I\n",
    "    I --> V\n",
    "    S --> V\n",
    "    V --> P\n",
    "    P -- No --> E\n",
    "    P -- Yes --> A\n",
    "\n",
    "    style P fill:#aaffd6,stroke:#333\n",
    "    style A stroke-dasharray: 5 5\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1f2ed5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ‚úÖ Pandera Quickstart\n",
    "\n",
    "### Create statistical types for your DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b725fcf4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/pandera-dev/pandera/master/docs/source/_static/pandera-logo.png\" width=\"125px\" style=\"margin: 0;\"/>\n",
    "\n",
    "<h2 style=\"margin-top: 0;\">Pandera</h2>\n",
    "\n",
    "#### An expressive and light-weight statistical typing tool for dataframes\n",
    "\n",
    "- Check the types and properties of dataframes\n",
    "- Easily integrate with existing data pipelines via function decorators\n",
    "- Synthesize data from schema objects for property-based testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb096f73",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Object-based API\n",
    "\n",
    "Defining a schema looks and feels like defining a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0838e347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandera as pa\n",
    "\n",
    "clean_data_schema = pa.DataFrameSchema(\n",
    "    columns={\n",
    "        \"continuous\": pa.Column(float, pa.Check.ge(0), nullable=True),\n",
    "        \"categorical\": pa.Column(str, pa.Check.isin([\"A\", \"B\", \"C\"]), nullable=True),\n",
    "    },\n",
    "    coerce=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e679ccf1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Class-based API\n",
    "\n",
    "Complex Types with Modern Python, Inspired by [pydantic](https://pydantic-docs.helpmanual.io/) and `dataclasses`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751e1d8c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from pandera.typing import DataFrame, Series\n",
    "\n",
    "class CleanData(pa.SchemaModel):\n",
    "    continuous: Series[float] = pa.Field(ge=0, nullable=True)\n",
    "    categorical: Series[str] = pa.Field(isin=[\"A\", \"B\", \"C\"], nullable=True)\n",
    "\n",
    "    class Config:\n",
    "        coerce = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcfd826",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Pandera comes in two flavors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a3710a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Pandera Raises Informative Errors\n",
    "\n",
    "Know Exactly What Went Wrong with Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6381569",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_data = pd.DataFrame({\n",
    "    \"continuous\": [\"-1.1\", \"4.0\", \"10.25\", \"-0.1\", \"5.2\"],\n",
    "    \"categorical\": [\"A\", \"B\", \"C\", \"Z\", \"X\"],\n",
    "})\n",
    "\n",
    "try:\n",
    "    CleanData.validate(raw_data, lazy=True)\n",
    "except pa.errors.SchemaErrors as exc:\n",
    "    display(exc.failure_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b524cafd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pandera Supports Schema Transformations/Inheritence\n",
    "\n",
    "#### Object-based API\n",
    "\n",
    "Dynamically transform schema objects on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab74604",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_schema = pa.DataFrameSchema(\n",
    "    columns={\n",
    "        \"continuous\": pa.Column(float),\n",
    "        \"categorical\": pa.Column(str),\n",
    "    },\n",
    "    coerce=True,\n",
    ")\n",
    "\n",
    "clean_data_schema.update_columns({\n",
    "    \"continuous\": {\"nullable\": True},\n",
    "    \"categorical\": {\"checks\": pa.Check.isin([\"A\", \"B\", \"C\"]), \"nullable\": True},\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f4b7fd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Class-based API\n",
    "\n",
    "Inherit from `pandera.SchemaModel` to Define Type Hierarchies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd30895",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawData(pa.SchemaModel):\n",
    "    continuous: Series[float]\n",
    "    categorical: Series[str]\n",
    "\n",
    "    class Config:\n",
    "        coerce = True\n",
    "\n",
    "class CleanData(RawData):\n",
    "    continuous = pa.Field(ge=0, nullable=True)\n",
    "    categorical = pa.Field(isin=[\"A\", \"B\", \"C\"], nullable=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f00d221",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Integrate Seamlessly with your Pipeline\n",
    "\n",
    "Use decorators to add IO checkpoints to the critical functions in your pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e80d32",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "@pa.check_types\n",
    "def fn(raw_data: DataFrame[RawData]) -> DataFrame[CleanData]:\n",
    "    return raw_data.assign(\n",
    "        continuous=lambda df: df[\"continuous\"].where(lambda x: x > 0, np.nan),\n",
    "        categorical=lambda df: df[\"categorical\"].where(lambda x: x.isin([\"A\", \"B\", \"C\"]), np.nan),\n",
    "    )\n",
    "\n",
    "\n",
    "fn(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d38ccd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generative Schemas\n",
    "\n",
    "Schemas that synthesize valid data under its constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414f4b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "CleanData.example(size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bd876b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Data Testing:** Test the functions that produce clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171c41d1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from hypothesis import given\n",
    "\n",
    "\n",
    "@given(RawData.strategy(size=5))\n",
    "def test_fn(raw_data):\n",
    "    fn(raw_data)\n",
    "\n",
    "\n",
    "def run_test_suite():\n",
    "    test_fn()\n",
    "    print(\"tests passed ‚úÖ\")\n",
    "\n",
    "\n",
    "run_test_suite()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f5e9ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scaling Pandera with Pandas' Friends üêºüåà‚ú®\n",
    "\n",
    "Pandera supports `dask`, `modin`, and `pyspark.pandas` dataframes to scale\n",
    "data validation to big data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e830af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e585a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Apply a single schema to a suite of dataframe-like objects\n",
    "\n",
    "#### `dask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7be0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "dask_dataframe = dd.from_pandas(raw_data, npartitions=1)\n",
    "\n",
    "try:\n",
    "    CleanData(dask_dataframe, lazy=True).compute()\n",
    "except pa.errors.SchemaErrors as exc:\n",
    "    display(exc.failure_cases.sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c230f57",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Apply a single schema to a suite of dataframe-like objects\n",
    "\n",
    "#### `modin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092dad47",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import modin.pandas as mpd\n",
    "\n",
    "modin_dataframe = mpd.DataFrame(raw_data)\n",
    "\n",
    "try:\n",
    "    CleanData(modin_dataframe, lazy=True)\n",
    "except pa.errors.SchemaErrors as exc:\n",
    "    display(exc.failure_cases.sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55d099c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Apply a single schema to a suite of dataframe-like objects\n",
    "\n",
    "#### `pyspark.pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b9154a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "pyspark.SparkContext().setLogLevel(\"OFF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a22087",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "pyspark_pd_dataframe = ps.DataFrame(raw_data)\n",
    "\n",
    "try:\n",
    "    CleanData(pyspark_pd_dataframe, lazy=True)\n",
    "except pa.errors.SchemaErrors as exc:\n",
    "    display(exc.failure_cases.sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dc62e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Meta Comment\n",
    "\n",
    "##### This presentation notebook is validated by pandera ü§Ø"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6190a04b",
   "metadata": {
    "tags": []
   },
   "source": [
    "![mindblown](https://media.giphy.com/media/xT0xeJpnrWC4XWblEk/giphy-downsized-large.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57aa3af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ‚å®Ô∏è Statistical Typing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e858be",
   "metadata": {},
   "source": [
    "#### Type systems help programmers (and machines!) reason about and write more robust code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73387e81",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "Number = Union[int, float]\n",
    "\n",
    "def add_and_double(x: Number, y: Number) -> Number:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fae15c7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Can you predict the outcome of these function calls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879aee9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_and_double(5, 2)\n",
    "add_and_double(5, \"hello\")\n",
    "add_and_double(11.5, -1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed4742c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Similarly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed4f8e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import pandera as pa\n",
    "from pandera.typing import DataFrame, Series\n",
    "\n",
    "class Inputs(pa.SchemaModel):\n",
    "    x: Series[int]\n",
    "    y: Series[int]\n",
    "\n",
    "    class Config:\n",
    "        coerce = True\n",
    "\n",
    "\n",
    "class Outputs(Inputs):\n",
    "    z: Series[int]\n",
    "        \n",
    "    @pa.dataframe_check\n",
    "    def custom_check(cls, df: DataFrame) -> Series:\n",
    "        return df[\"z\"] == (df[\"x\"] + df[\"y\"]) * 2\n",
    "    \n",
    "    \n",
    "@pa.check_types\n",
    "def add_and_double(raw_data: DataFrame[Inputs]) -> DataFrame[Outputs]:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4f45a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ü§î What's Statistical Typing?\n",
    "\n",
    "> **Statistical typing** extends primitive data types with additional semantics\n",
    "> about the _properties held by a collection of data points_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d0b0ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Consider a single data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cb8cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_point = {\"square_footage\": 700, \"nbedrooms\": 1, \"price\": 500_000}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1250ed2e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Primitive datatypes\n",
    "- Value range\n",
    "- Allowable values\n",
    "- Regex string match\n",
    "- Nullability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2beb0b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Now consider a collection data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881e233c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_points = [\n",
    "    {\"square_footage\": 700, \"nbedrooms\": 1, \"price\": 500_000},\n",
    "    {\"square_footage\": 1000, \"nbedrooms\": 2, \"price\": 750_000},\n",
    "    {\"square_footage\": 3000, \"nbedrooms\": 4, \"price\": 1_000_000},\n",
    "    ...\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe0d878",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Apply atomic checks at scale\n",
    "- Uniqueness\n",
    "- Monotonicity\n",
    "- Mean, median, standard deviation\n",
    "- Fractional checks, e.g. 90% of data points are not null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7188cbc7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pandera is a Statistical Type System Geared Towards Data Science\n",
    "\n",
    "Statistical types are defined with multiple layers üßÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7ede45",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> **primitive data types**: `int`, `float`, `bool`, `str`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154fe0bb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> **deterministic properties**: domain of possible values, e.g. `x >= 0`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7a62b0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> **probabilistic properties**: distributions that apply to the variable and their sufficient statistics, e.g. `mean`,\n",
    "  `standard deviation`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46f438a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# üìä Example 1: Validate your Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7857ae1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset: California Housing\n",
    "\n",
    "A dataset containing ~20,000 samples where each row is a California district and\n",
    "each column is an aggregate statistic about that district."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed814318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing_data = fetch_california_housing(as_frame=True).frame\n",
    "housing_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950f6e95",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### With a cursory glance at the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abf3e0f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "housing_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06fe502",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### We can start defining a basic schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa96cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HousingData(pa.SchemaModel):\n",
    "\n",
    "    # features\n",
    "    MedInc: Series[float] = pa.Field(in_range={\"min_value\": 0, \"max_value\": 100})\n",
    "    HouseAge: Series[float] = pa.Field(in_range={\"min_value\": 0, \"max_value\": 100})\n",
    "    AveRooms: Series[float] = pa.Field(in_range={\"min_value\": 0, \"max_value\": 1_000})\n",
    "    AveBedrms: Series[float] = pa.Field(in_range={\"min_value\": 0, \"max_value\": 100})\n",
    "    Population: Series[float] = pa.Field(in_range={\"min_value\": 0, \"max_value\": 100_000})\n",
    "    AveOccup: Series[float] = pa.Field(in_range={\"min_value\": 0, \"max_value\": 10_000})\n",
    "    Latitude: Series[float] = pa.Field(in_range={\"min_value\": -90, \"max_value\": 90})\n",
    "    Longitude: Series[float] = pa.Field(in_range={\"min_value\": -180, \"max_value\": 180})\n",
    "\n",
    "    # target variable! üéØ\n",
    "    MedHouseVal: Series[float] = pa.Field(in_range={\"min_value\": 0, \"max_value\": 100})\n",
    "\n",
    "    class Config:\n",
    "        coerce = True\n",
    "\n",
    "\n",
    "@pa.check_types\n",
    "def read_data() -> DataFrame[HousingData]:\n",
    "    return fetch_california_housing(as_frame=True).frame\n",
    "\n",
    "\n",
    "housing_data = read_data()\n",
    "print(\"validation passed ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f969105",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Analysis Pipeline\n",
    "\n",
    "Hypothesis: Median income is positively correlated with Median House Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0313a076",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def analyze_data(housing_data, var1, var2):\n",
    "    correlation_coef = housing_data[[var1, var2]].corr().at[var1, var2]\n",
    "    display(Markdown(f\"Pearson correlation coefficient = {correlation_coef:0.06f}\"))\n",
    "    housing_data.plot.scatter(var1, var2, s=1, alpha=0.5)\n",
    "\n",
    "analyze_data(housing_data, \"MedInc\", \"MedHouseVal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d8a1c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Bake in statistical hypothesis testing into your pipeline\n",
    "\n",
    "Easily create re-usable custom checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a93b66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "import pandera.extensions as extensions\n",
    "\n",
    "@extensions.register_check_method(\n",
    "    statistics=[\"var1\", \"var2\", \"alpha\"],\n",
    "    supported_types=[pd.DataFrame]\n",
    ")\n",
    "def is_positively_correlated(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    var1: str,\n",
    "    var2: str,\n",
    "    alpha: float = 0.01,\n",
    "):\n",
    "    \"\"\"Perform Pearson correlation hypothesis test.\"\"\"\n",
    "\n",
    "    r, pvalue = pearsonr(df[var1], df[var2])\n",
    "    passed = r > 0 and pvalue <= alpha\n",
    "\n",
    "    pretty_pvalue = np.format_float_scientific(pvalue)\n",
    "    if passed:\n",
    "        print(f\"‚úÖ {var1} is positively correlated with {var2} with r = {r:0.04f}; pvalue = {pretty_pvalue}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {var1} not correlated with {var2} with with r = {r:0.04f}; pvalue = {pretty_pvalue}\")\n",
    "\n",
    "    return passed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fe0d65",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Dynamically create schemas as statistical hypothesis validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e041ab75",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_data(housing_data, var1: str, var2: str):\n",
    "\n",
    "    class HousingDataHypothesis(HousingData):\n",
    "        class Config:\n",
    "            coerce = True\n",
    "            is_positively_correlated = {\n",
    "                \"var1\": var1,\n",
    "                \"var2\": var2,\n",
    "                \"alpha\": 0.01,\n",
    "            }\n",
    "\n",
    "    housing_data = HousingDataHypothesis.validate(housing_data)\n",
    "    correlation_coef = housing_data[[var1, var2]].corr().at[var1, var2]\n",
    "    display(Markdown(f\"Pearson correlation coefficient = {correlation_coef:0.06f}\"))\n",
    "    housing_data.plot.scatter(var1, var2, s=1, alpha=0.5)\n",
    "\n",
    "\n",
    "analyze_data(housing_data, \"MedInc\", \"MedHouseVal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee7cc38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### The Analysis Pipeline\n",
    "\n",
    "Every time this runs, pandera makes sure all the assumptions encoded in the schemas\n",
    "hold true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aff5af",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def run_analysis_pipeline(var1: str, var2: str):\n",
    "    data = read_data()\n",
    "    analyze_data(data, var1, var2)\n",
    "\n",
    "\n",
    "run_analysis_pipeline(\"MedInc\", \"MedHouseVal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a194b090",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ü§ñ Example 2: Validate your Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27101797",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prediction Task:\n",
    "\n",
    "From all the features, predict the median house value target `MedHouseVal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e55bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# ‚ö†Ô∏è This is the most critical part to check\n",
    "@pa.check_types\n",
    "def split_data(\n",
    "    data: DataFrame[HousingData],\n",
    "    test_size: float = 0.2,\n",
    ") -> Tuple[DataFrame[HousingData], DataFrame[HousingData]]:\n",
    "    return train_test_split(data, test_size=test_size)\n",
    "\n",
    "\n",
    "# üëâ Notice that I don't use @pa.check_types here\n",
    "def parse_data(data: DataFrame[HousingData], target: str) -> Tuple[DataFrame[HousingData], pd.Series]:\n",
    "    features = [column for column in data if column != target]\n",
    "    return data[features], data[target]\n",
    "\n",
    "\n",
    "# üîΩ At this point onward the type annotations are for type linters like mypy\n",
    "def train(features: pd.DataFrame, target: pd.Series) -> LinearRegression:\n",
    "    model = LinearRegression()\n",
    "    return model.fit(features, target)\n",
    "\n",
    "\n",
    "def evaluate(model: LinearRegression, features: pd.DataFrame, target: pd.Series) -> float:\n",
    "    prediction = model.predict(features)\n",
    "    return r2_score(target, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8613e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Running a validated training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d21053",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def run_training_pipeline(data: pd.DataFrame, target: str):\n",
    "    train_data, test_data = split_data(data)\n",
    "    train_features, train_target = parse_data(train_data, target)\n",
    "\n",
    "    # train a model\n",
    "    model = train(train_features, train_target)\n",
    "\n",
    "    # evaluate\n",
    "    train_r2 = evaluate(model, train_features, train_target)\n",
    "    test_r2 = evaluate(model, *parse_data(test_data, target))\n",
    "\n",
    "    return model, train_r2, test_r2\n",
    "\n",
    "\n",
    "model, train_r2, test_r2 = run_training_pipeline(read_data(), \"MedHouseVal\")\n",
    "print(f\"üèãÔ∏è‚Äç‚ôÇÔ∏è Train R^2 score: {train_r2:0.6f}\")\n",
    "print(f\"üìù Test R^2 score: {test_r2:0.6f}\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7c7a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Unit testing a training pipeline\n",
    "\n",
    "Synthesize mock training data so that you don't have to hand-craft dataframes ü§Ø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4df678",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from hypothesis import settings\n",
    "\n",
    "prediction_schema = pa.SeriesSchema(float, nullable=False)\n",
    "\n",
    "\n",
    "@given(HousingData.strategy(size=10))\n",
    "@settings(max_examples=3)\n",
    "def test_run_training_pipeline(data):\n",
    "    target = \"MedHouseVal\"\n",
    "    model, *_ = run_training_pipeline(data, target)\n",
    "    features, _ = parse_data(data, target)\n",
    "    predictions = pd.Series(model.predict(features))\n",
    "\n",
    "    # validate predictions\n",
    "    prediction_schema(predictions)\n",
    "\n",
    "\n",
    "def run_test_suite():\n",
    "    test_run_training_pipeline()\n",
    "    print(\"‚úÖ training pipeline test suite passed!\")\n",
    "\n",
    "\n",
    "run_test_suite()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd89f634",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ‚≠êÔ∏è Conclusion: How can I start using Pandera in my work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66f1b2b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Incrementally adopt `pandera` into your workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd455603",
   "metadata": {
    "slideshow": {
     "slide_type": "fragement"
    }
   },
   "source": [
    "> üß† ‚Üí üìù Encode the domain knowledge that you build up during the development and exploration process into schemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f889fe86",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> ü•æ‚ú® If you're in a hurry, use [`pandera.infer_schema`](https://pandera.readthedocs.io/en/stable/schema_inference.html)\n",
    "> to bootstrap a schema and refine it over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90639cf2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> ‚ùóÔ∏è Identify the critical functions in your data processing pipeline and add `@pa.check_types` decorators as\n",
    "checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae9fe52",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> üî© Codify data quality checks that are specific to your problem domain by creating reusable custom validation rules\n",
    "> via `@pandera.extensions.register_check_method`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7c61ee",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> üîÑ Reuse schemas for runtime validation or test-time validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fc0520",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> ü§© Be more confident in the correctness of your analysis/model with programmatically enforced, self-documenting code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b59b96",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## üõ£ Future Roadmap\n",
    "\n",
    "- üìè **Extensibility:** getting support for other schema formats and data container objects e.g.\n",
    "  `xarray`, `jsonschema`, `cudf`, `pyarrow`, and an extension API for arbitrary data containers.\n",
    "- üíª **UX:** better error-reporting, more built-in checks, statistical hypothesis checks, conditional validation, and more!\n",
    "- ü§ù **Interoperability:** tighter integrations with the python ecosystem, e.g. `fastapi`, `pydantic`, `pytest`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99531f44",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Join the Community!\n",
    "\n",
    "![badge](https://img.shields.io/github/stars/pandera-dev/pandera?style=social)\n",
    "[![badge](https://img.shields.io/pypi/pyversions/pandera.svg)](https://pypi.python.org/pypi/pandera/)\n",
    "[![badge](https://img.shields.io/pypi/v/pandera.svg)](https://pypi.org/project/pandera/)\n",
    "![badge](https://img.shields.io/github/contributors/pandera-dev/pandera)\n",
    "[![badge](https://pepy.tech/badge/pandera)](https://pepy.tech/project/pandera)\n",
    "[![badge](https://pepy.tech/badge/pandera/month)](https://pepy.tech/project/pandera)\n",
    "[![badge](https://img.shields.io/badge/discord-chat-purple?color=%235765F2&label=discord&logo=discord)](https://discord.gg/vyanhWuaKB)\n",
    "\n",
    "\n",
    "- **Twitter**: [@cosmicbboy](https://twitter.com/cosmicBboy)\n",
    "- **Discord**: https://discord.gg/vyanhWuaKB\n",
    "- **Email**: [niels@union.ai](mailto:niels@union.ai)\n",
    "- **Repo**: https://github.com/unionai-oss/pandera\n",
    "- **Docs**: https://pandera.readthedocs.io\n",
    "- **Contributing Guide**: https://pandera.readthedocs.io/en/stable/CONTRIBUTING.html\n",
    "- **Become a Sponsor**: https://github.com/sponsors/cosmicBboy"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "pandera-presentations",
   "language": "python",
   "name": "pandera-presentations"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
