<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link rel="stylesheet" href="/assets/css/style.css?v=a2dc1c8c07e17a28bafead8681abb3500f5b4a1a">
    <link rel="stylesheet" type="text/css" href="/assets/css/print.css" media="print">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Scipy 2020 Conference Presentation Notes | Pandera Presentations</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Scipy 2020 Conference Presentation Notes" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Talks about pandera" />
<meta property="og:description" content="Talks about pandera" />
<link rel="canonical" href="http://localhost:4000/presentation_notes/20200505_scipy_conference.html" />
<meta property="og:url" content="http://localhost:4000/presentation_notes/20200505_scipy_conference.html" />
<meta property="og:site_name" content="Pandera Presentations" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Scipy 2020 Conference Presentation Notes" />
<script type="application/ld+json">
{"@type":"WebPage","headline":"Scipy 2020 Conference Presentation Notes","description":"Talks about pandera","url":"http://localhost:4000/presentation_notes/20200505_scipy_conference.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body>
    <div id="container">
      <div class="inner">

        <header>
          <h1>Scipy 2020 Conference Presentation Notes</h1>
          <h2>Talks about pandera</h2>
        </header>
        <section id="downloads" class="clearfix">
          
	
        </section>
        <hr>
        <section id="main_content">
          <h1 id="scipy-2020-conference-presentation-notes">Scipy 2020 Conference Presentation Notes</h1>

<h2 id="pandera-statistical-data-validation-of-pandas-dataframes"><code class="language-plaintext highlighter-rouge">pandera</code>: Statistical Data Validation of Pandas Dataframes</h2>

<p>Hi everyone, I‚Äôm Niels Bantilan, and I‚Äôm very excited to have the chance to
talk to you at the scipy conference this year. I‚Äôm a machine learning engineer
at Talkspace, which is a mental health platform that connects people with
therapists. My work centers around using NLP in clinical applications of
machine learning, but today I want to talk to you about statistical data
validation of pandas dataframes.</p>

<h2 id="whats-a-dataframe">What‚Äôs a Dataframe?</h2>

<p>For those of you who‚Äôre not so familiar with pandas or dataframes, a dataframe
is basically a table that you can manipulate programmatically, and <code class="language-plaintext highlighter-rouge">pandas</code>
is one of the de facto tools for the manipulation and analysis of tabular data
in Python.</p>

<p>Sometimes you may want to explicitly validate the contents and properties of a
dataframe because you care about data quality.</p>

<h2 id="whats-data-validation">What‚Äôs Data Validation?</h2>

<p>Data validation is simply the act of falsifying data against explicit
assumptions for some downstream purpose.</p>

<p>Consider the statement ‚Äúall swans are white‚Äù. To verify this statement, we‚Äôd
have to go out and find all the swans and find that all of them are white,
which would be virtually impossible. Another approach we can take is to
find the first instance of a black swan to falsify the statement.</p>

<h2 id="why-do-i-need-it">Why Do I Need it?</h2>

<p>Practically speaking, I often find it difficult to reason about and debug
data processing pipelines, and I know it‚Äôs good practice to ensure
data quality, especially when the end product informs business decisions,
supports scientific findings, or generates predictions in a production setting.</p>

<h2 id="everyone-has-a-personal-relationship-with-their-dataframes">Everyone has a personal relationship with their dataframes</h2>

<p>And finally, as I like to say, everyone has a personal relationship with their
dataframes. So you might know it intimately at the time, but future you or
other maintainers of the codebase won‚Äôt be able to understand the workings of
your data processing pipeline in a few months.</p>

<p>So to illustrate these points, I‚Äôd like to tell you a story, involving you!</p>

<h2 id="one-day-you-encounter-an-error-log-trail-and-decide-to-follow-it">One day, you encounter an error log trail and decide to follow it‚Ä¶</h2>

<p>The error is a <code class="language-plaintext highlighter-rouge">TypeError</code> involving a multiplication operation.</p>

<h2 id="and-you-find-yourself-at-the-top-of-a-function">And you find yourself at the top of a function‚Ä¶</h2>

<p>Called <code class="language-plaintext highlighter-rouge">process_data</code></p>

<h2 id="you-look-around-and-see-some-hints-of-what-had-happened">You look around, and see some hints of what had happened‚Ä¶</h2>

<p>Namely, that the function is trying to compute <code class="language-plaintext highlighter-rouge">weekly_income</code> from
<code class="language-plaintext highlighter-rouge">hours_worked</code> and <code class="language-plaintext highlighter-rouge">wage_per_hour</code></p>

<h2 id="you-sort-of-know-whats-going-on-but-you-want-to-take-a-closer-look">You sort of know what‚Äôs going on, but you want to take a closer look!</h2>

<p>So you insert a breakpoint</p>

<h2 id="and-you-find-some-funny-business-going-on">And you find some funny business going on‚Ä¶</h2>

<p>You notice a negative value, which is a problem in itself, but you know that this
can‚Äôt be the cause of the <code class="language-plaintext highlighter-rouge">TypeError</code>. You look a little more closely at the
type of the <code class="language-plaintext highlighter-rouge">hours_worked</code> column, and notice a string in one of the elements.</p>

<h2 id="you-squash-the-bug-and-add-documentation-for-the-next-weary-traveler-who-happens-upon-this-code">You squash the bug and add documentation for the next weary traveler who happens upon this code.</h2>

<p>Making sure that the columns are the expected type, and the <code class="language-plaintext highlighter-rouge">hours_worked</code>
values must be positive, converting negative values into <code class="language-plaintext highlighter-rouge">nan</code>s. Doing this,
you merge your commit, and all is well with the world.</p>

<h2 id="a-few-months-later">A few months later‚Ä¶</h2>

<h2 id="you-find-yourself-at-a-familiar-function-but-it-looks-a-little-different-from-when-you-left-it">You find yourself at a familiar function, but it looks a little different from when you left it‚Ä¶</h2>

<p>The type coercion logic is gone, and you see these function decorators on
the <code class="language-plaintext highlighter-rouge">process_data</code> function.</p>

<h2 id="you-look-above-and-see-what-in_schema-and-out_schema-are-finding-a-note-that-a-fellow-traveler-has-left-for-you">You look above and see what in_schema and out_schema are, finding a NOTE that a fellow traveler has left for you.</h2>

<p>This way, it‚Äôs immediately clear what should be in the <code class="language-plaintext highlighter-rouge">df</code> variable coming in
and out of <code class="language-plaintext highlighter-rouge">process_data</code>.</p>

<h2 id="moral-of-the-story">Moral of the Story</h2>

<p>The better you can reason about the contents of a dataframe, the faster you can debug.</p>

<p>The faster you can debug, the sooner you can focus on the tasks that you care about.</p>

<h2 id="outline">Outline</h2>

<p>In the rest of this talk, I‚Äôd like to take you a little more deeply
into what data validation means in theory and practice, briefly introduce
<code class="language-plaintext highlighter-rouge">pandera</code>, and take you through a case study that uses the library on a
real-world dataset. I‚Äôll then wrap up with a bit of a preview on experimental
features available today and some of the items in the roadmap for future
development.</p>

<h2 id="data-validation-in-theory-and-practice">Data Validation in Theory and Practice</h2>

<p>According to the European Statistical System, data validation is an activity in
which it is verified whether or not a combination of values is a member of a
set of acceptable value combinations.</p>

<h2 id="data-validation-in-theory-and-practice-1">Data Validation in Theory and Practice</h2>

<p>More formally, we can relate this definition to the notion of falsifiability.
Data validation is the act of defining a validation function <code class="language-plaintext highlighter-rouge">v</code> that takes
some data <code class="language-plaintext highlighter-rouge">x</code> as input and outputs either <code class="language-plaintext highlighter-rouge">True</code> or <code class="language-plaintext highlighter-rouge">False</code>. As van der Loo
et al. point out, <code class="language-plaintext highlighter-rouge">v</code> needs to be a surjective function, which is just a
fancy way of saying there must exists at least one value of <code class="language-plaintext highlighter-rouge">x</code> that maps onto
<code class="language-plaintext highlighter-rouge">True</code> at at least another value of <code class="language-plaintext highlighter-rouge">x</code> that maps onto <code class="language-plaintext highlighter-rouge">False</code>.</p>

<h2 id="case-1-unfalsifiable">Case 1: Unfalsifiable</h2>

<p>To see why, consider the following two cases. In case one, <code class="language-plaintext highlighter-rouge">v</code> always returns
<code class="language-plaintext highlighter-rouge">True</code>, making the function unfalsifiable. This is effectively like not having
any validation function at all, since all it does is continue the execution of
your program. In case two, <code class="language-plaintext highlighter-rouge">v</code> always returns <code class="language-plaintext highlighter-rouge">False</code>, which is like saying
‚Äúmy dataframe has an infinite number of rows and columns‚Äù. There‚Äôs no
practical implementation of such a function, making it unverifiable. You
really need this surjectivity property in order to meaningfully distinguish
between valid and invalid data.</p>

<h2 id="types-of-validation-rules">Types of Validation Rules</h2>

<p>So within this space of meaningful validation functions, one way to think about
validation rules is in terms of technical checks, which have to do with 
properties of the data structure, e.g. <code class="language-plaintext highlighter-rouge">income</code> is a numeric variable, and
domain-specific checks, which have to do with properties specific to the
topic under study, e.g. the <code class="language-plaintext highlighter-rouge">age</code> variable must be in the range <code class="language-plaintext highlighter-rouge">0</code> and <code class="language-plaintext highlighter-rouge">120</code>.</p>

<p>Another way of thinking about validation rules is in statistical terms. On
one hand, you might define deterministic checks, which express hard-coded
logical rules, and on the other you might want to define probabilistic checks,
which explicitly incorporate randomness and distributional variability. In
these examples, you can see that we‚Äôre roughly saying the same thing about
the mean age being between <code class="language-plaintext highlighter-rouge">30</code> and <code class="language-plaintext highlighter-rouge">40</code> years, but the probabilistic check
incorporates information about the sample size and variability of the
sample distribution to assert that the confidence bounds of the distribution
are between <code class="language-plaintext highlighter-rouge">30</code> and <code class="language-plaintext highlighter-rouge">40</code>.</p>

<h2 id="statistical-type-safety">Statistical Type Safety</h2>

<p>Considering these kinds of statistical validation rules leads us to think
about statistical type safety, which would be like logical type safety but
applied to the distributional properties of data to ensure that statistical
operations on those data are valid, for example checking whether:</p>

<ul>
  <li>the training samples are IID</li>
  <li>the features are not multicolinear</li>
  <li>the variance of a feature is greater than some threshold</li>
  <li>or the training and test set labels are drawn from the same distribution</li>
</ul>

<h2 id="data-validation-workflow-in-practice">Data Validation Workflow in Practice</h2>

<p>Practically speaking, the data validation workflow may seem familiar. You
start with a goal in your head as to what you want the data to look like, you
write some code, and check if it‚Äôs good enough for the purposes of your
analysis. If it is, you get on with it, but if it isn‚Äôt you rinse and repeat
the process.</p>

<h2 id="user-story">User Story</h2>

<p>Having experienced this loop many times and writing my own ad hoc validation
code, this is the user story I started out with.</p>

<blockquote>
  <p>As a machine learning engineer who uses pandas every day, I want a data
validation tool that‚Äôs intuitive, flexible, customizable, and easy to
integrate into my ETL pipelines so that I can spend less time worrying about
the correctness of a dataframe‚Äôs contents and more time training models.</p>
</blockquote>

<h2 id="introducing-pandera">Introducing <code class="language-plaintext highlighter-rouge">pandera</code></h2>

<p>What I ended up with is a design-by-contract data validation library that
exposes an intuitive API for expressing dataframe schemas.</p>

<h2 id="refactoring-the-validation-function">Refactoring the Validation Function</h2>

<p>From the very start, I took the que from existing data validation libraries
in Python and formulated the schema function <code class="language-plaintext highlighter-rouge">s</code> to behave like this: it takes
a validation function and data as input and outputs the data itself if the
validation function evaluates to <code class="language-plaintext highlighter-rouge">True</code> but raises an <code class="language-plaintext highlighter-rouge">Exception</code> otherwise.</p>

<h2 id="why">Why?</h2>

<p>The main reason for this is compositionality. If we have some data processing
function <code class="language-plaintext highlighter-rouge">f</code>, we can compose it with our schema in various ways. We can:</p>

<ul>
  <li>validate the raw data before it‚Äôs processed by <code class="language-plaintext highlighter-rouge">f</code></li>
  <li>validate the output of <code class="language-plaintext highlighter-rouge">f</code></li>
  <li>and my favorite, make a data validation sandwich</li>
</ul>

<h2 id="architecture">Architecture</h2>

<p>Pictorially, this shows how you‚Äôd use the <code class="language-plaintext highlighter-rouge">pandera</code> schema to interleave data
validation with data processing logic.</p>

<h2 id="case-study-fatal-encounters-dataset">Case Study: Fatal Encounters Dataset</h2>

<p>And to give you a hands on sense of how you might integrate this tool into your
workflow, I wanted to take you through an analysis of the fatal encounters
dataset. I‚Äôve been following this is amazing work for the past couple of years,
headed by D. Brian Burghart and other collaborators to compile the most
comprehensive national database of people killed during interactions with law
enforcement. In light of recent events, I wanted to take this opportunity to
point people to this dataset because, unsurprisingly, it‚Äôs hard to find
official records of such encounters online.</p>

<h2 id="birds-eye-view">Bird‚Äôs Eye View</h2>

<p>To give you an overview of this dataset, there are 26,000+ records of law
enforcement encounters that lead to death dating back to the year 2000, where
each record contains the demographics of the decedent, the cause and location of
death, a description of the encounter, and the court disposition of the case.</p>

<p>The question I want to ask in this analysis is:</p>

<h2 id="what-factors-are-most-predictive-of-the-court-ruling-a-case-as-accidental">What factors are most predictive of the court ruling a case as ‚ÄúAccidental‚Äù?</h2>

<p>To give you a sense of some of the circumstances that lead to these accidental
deaths, here are a few examples.</p>

<blockquote>
  <p>Undocumented immigrant Roberto Chavez-Recendiz was fatally shot while Rivera was arresting him along with his brother and brother-in-law for allegedly being in the United States without proper documentation. The shooting was ‚Äúpossibly the result of what is called a ‚Äòsympathetic grip,‚Äô where one hand reacts to the force being used by the other,‚Äù Chief Criminal Deputy County Attorney Rick Unklesbay wrote in a letter outlining his review of the shooting. The Border Patrol officer had his pistol in his hand as he took the suspects into custody. He claimed the gun fired accidentally.</p>
</blockquote>

<blockquote>
  <p>Andrew Lamar Washington died after officers tasered him 17 times within three minutes.</p>
</blockquote>

<blockquote>
  <p>Biddle and his brother, Drake Biddle, were fleeing from a Nashville Police Department officer at a high rate of speed when a second Nashville Police Department officer, James L. Steely, crashed into him head-on.</p>
</blockquote>

<h2 id="caveats">Caveats</h2>

<p>While I let the emotional impact of these examples sink in, some caveats.</p>

<p>I do want to emphasize that, as mentioned on the website, the records in this
dataset are the most comprehensive to date, but by no means is it a finished
project. Biases may be lurking everywhere!</p>

<p>I don‚Äôt have any domain expertise in criminal justice!</p>

<p>The main purpose here is to showcase the capabilities of pandera.</p>

<p>And also, now that I‚Äôve built this tool, I want to validate all the things.</p>

<h2 id="notebook-is-available-here">Notebook is Available Here</h2>

<p>Before I dive in, if anyone‚Äôs curious to take a look at this presentation and
this analysis, I‚Äôve set up some binder notebooks that you can visit via these
bitly links to run and play around with the code as you see fit.</p>

<h2 id="read-the-data">Read the Data</h2>

<p>The first step in any analysis is to read in the data. You can inspect the first
couple of rows to get a first impression of what‚Äôs in there.</p>

<h2 id="clean-up-column-names">Clean up column names</h2>

<p>Before proceed further, we should probably clean up column names to make our
analysis more readable, and this is where it might be a good idea to define
a minimal schema definition.</p>

<h2 id="minimal-schema-definition">Minimal Schema Definition</h2>

<p>All we‚Äôre saying here is these are the columns that we‚Äôre interested in
modeling. Note that the <code class="language-plaintext highlighter-rouge">dispositions_exclusions</code> column, which we‚Äôre going to
use to derive our target variable, shouldn‚Äôt be <code class="language-plaintext highlighter-rouge">nullable</code>.</p>

<p>We can use this schema in our <code class="language-plaintext highlighter-rouge">clean_columns</code> function by just <code class="language-plaintext highlighter-rouge">pipe</code>ing it
at the end of the method chain.</p>

<h3 id="sidebar">Sidebar</h3>

<p>If for some reason a column specified in our schema isn‚Äôt there, <code class="language-plaintext highlighter-rouge">pandera</code>
will raise a <code class="language-plaintext highlighter-rouge">SchemaError</code> complaining about it.</p>

<h2 id="explore-the-data-with-pandas-profiling">Explore the Data with <code class="language-plaintext highlighter-rouge">pandas-profiling</code></h2>

<p>Now that we‚Äôve cleaned up the column names let‚Äôs use <code class="language-plaintext highlighter-rouge">pandas-profiling</code> to
get a better sense of the distributions in this dataset. Just to give you
a sense of what‚Äôs in the <code class="language-plaintext highlighter-rouge">dispositions_exclusions</code> column, these are the
categories related to the court decisions for each record.</p>

<h2 id="declare-the-training-data-schema">Declare the Training Data Schema</h2>

<p>After doing some data exploration, you might arrive at a richer schema for
the training data, including assumptions about the <code class="language-plaintext highlighter-rouge">age</code> range, acceptable
values in the <code class="language-plaintext highlighter-rouge">gender</code>, <code class="language-plaintext highlighter-rouge">race</code>, and <code class="language-plaintext highlighter-rouge">cause_of_death</code> columns, and the types
of each variable, which we want to ensure with the <code class="language-plaintext highlighter-rouge">coerce</code> keyword argument.
Because we want to build a model predicting whether a case was ruled as
<code class="language-plaintext highlighter-rouge">accidental</code>, we‚Äôll need to derive this from the <code class="language-plaintext highlighter-rouge">dispositions_exclusions</code>
column.</p>

<h2 id="serialize-schema-to-yaml-format">Serialize schema to yaml format</h2>

<p>As a nifty feature of <code class="language-plaintext highlighter-rouge">pandera</code>, we can serialize the schema as a <code class="language-plaintext highlighter-rouge">yaml</code> file
so we can inspect the schema in more detail.</p>

<h2 id="clean-the-data">Clean the Data</h2>

<p>Next, we want to clean and normalize the data values before modeling it. This
involves cleaning strings, binarizing certain variables, and, most importantly,
deriving our target column <code class="language-plaintext highlighter-rouge">disposition_accidental</code>. Another thing to note
here is that we‚Äôre removing any records that are <code class="language-plaintext highlighter-rouge">unreported</code>, <code class="language-plaintext highlighter-rouge">unknown</code>,
<code class="language-plaintext highlighter-rouge">pending</code> or that were ruled <code class="language-plaintext highlighter-rouge">suicide</code>. The reason for this last criterion is
to model cases where law enforcement was more directly linked to the death
event.</p>

<h2 id="add-the-data-validation-">Add the data validation ü•™</h2>

<p>To ensure that our <code class="language-plaintext highlighter-rouge">clean_data</code> function does what it‚Äôs supposed to do, we
can make a data validation sandwich by using the <code class="language-plaintext highlighter-rouge">check_input</code> and
<code class="language-plaintext highlighter-rouge">check_output</code> decorators.</p>

<h2 id="valueerror-unable-to-coerce-column-to-schema-data-type">ValueError: Unable to coerce column to schema data type</h2>

<p>So if we try to apply the <code class="language-plaintext highlighter-rouge">clean_data</code> function as it is on the raw data, we
can an error, which is actually an error that I came across while making this
presentation. The message could be a little clearer, but I can tell you right
now that it‚Äôs because the <code class="language-plaintext highlighter-rouge">age</code> column is a <code class="language-plaintext highlighter-rouge">str</code> and not a numeric variable.</p>

<h2 id="normalize-the-age-column">Normalize the age column</h2>

<p>So let‚Äôs define a <code class="language-plaintext highlighter-rouge">normalize_age</code> function that converts the values in this
column to a float representing age in years. I‚Äôve abstracted away a few of the
details for your convenience.</p>

<h2 id="apply-normalize_age-inside-the-clean_data-function">Apply normalize_age inside the clean_data function.</h2>

<p>And now we can insert the <code class="language-plaintext highlighter-rouge">normalize_age</code> function somewhere in the middle of our
<code class="language-plaintext highlighter-rouge">clean_data</code> function.</p>

<h2 id="create-training-set">Create Training Set</h2>

<p>And with that change we‚Äôve fulfilled the contract defined in
<code class="language-plaintext highlighter-rouge">training_data_schema</code>!</p>

<h3 id="sidebar-1">Sidebar</h3>

<p>If, for some reason, the data gets corrupted, <code class="language-plaintext highlighter-rouge">Check</code> failure cases are
reported as a dataframe indexed by failure case value. The <code class="language-plaintext highlighter-rouge">index</code> column
contains a list of indexes in the invalid dataframe for a particular failure
case, and the <code class="language-plaintext highlighter-rouge">count</code> summarizes how many failure cases there were in the
dataframe of a particular value.</p>

<h3 id="fine-grained-debugging">Fine-grained debugging</h3>

<p>You can get finer-grained debugging by catching the <code class="language-plaintext highlighter-rouge">SchemaError</code> and
accessing the invalid <code class="language-plaintext highlighter-rouge">data</code> attribute, and you can also inspect the
<code class="language-plaintext highlighter-rouge">failure_cases</code> attribute, which is itself a dataframe referencing the <code class="language-plaintext highlighter-rouge">index</code>
in the invalid data and value causing the <code class="language-plaintext highlighter-rouge">SchemaError</code>.</p>

<h2 id="summarize-the-data">Summarize the Data</h2>

<p>One summary statistic we‚Äôd be interested in looking at is the class balance of
the target variable. We can see here that it‚Äôs 2.75%, which means the training
set is highly imbalanced‚Ä¶</p>

<p>To create a validation rule for this, we can use the <code class="language-plaintext highlighter-rouge">Hypothesis</code> class
to test that the proportion of <code class="language-plaintext highlighter-rouge">True</code> values in the <code class="language-plaintext highlighter-rouge">disposition_accidental</code>
column is equal 2.75% with an alpha value of 1%.</p>

<h2 id="prepare-training-and-test-sets">Prepare Training and Test Sets</h2>

<p>Now that we have our clean training data, it‚Äôs time to split it into our training
and test sets. Here I‚Äôd like to highlight a few features in <code class="language-plaintext highlighter-rouge">pandera</code>. First,
we can define <code class="language-plaintext highlighter-rouge">SeriesSchema</code> for our target variable, which works a lot like
<code class="language-plaintext highlighter-rouge">Column</code> schemas but it operates exclusively on pandas <code class="language-plaintext highlighter-rouge">Series</code> objects.</p>

<p>The second feature is that we can create modified copies of
<code class="language-plaintext highlighter-rouge">DataFrameSchema</code>s by calling particular methods. In this case, we‚Äôre going to
use the <code class="language-plaintext highlighter-rouge">remove_columns</code> method to create a <code class="language-plaintext highlighter-rouge">feature_schema</code>.</p>

<p>The third feature I want to highlight is the flexibility of the <code class="language-plaintext highlighter-rouge">check_input</code>
and <code class="language-plaintext highlighter-rouge">check_output</code> decorators. As you can see, the <code class="language-plaintext highlighter-rouge">split_training_data</code>
function should return a tuple of four arrays. We can specify an integer index
in <code class="language-plaintext highlighter-rouge">check_output</code> as the second argument to specify the pandas data structure
we want to verify with a particular schema. Here we‚Äôre saying we want to
validate the first two outputs with the <code class="language-plaintext highlighter-rouge">feature_schema</code>, and the last two
outputs with the <code class="language-plaintext highlighter-rouge">target_schema</code>.</p>

<h2 id="model-the-data">Model the Data</h2>

<p>Now it‚Äôs time to model the data. We‚Äôre going to use sklearn to do this, in
particular we‚Äôll make a <code class="language-plaintext highlighter-rouge">Pipeline</code> using the <code class="language-plaintext highlighter-rouge">ColumnTransformer</code> to
numericalize the features and a <code class="language-plaintext highlighter-rouge">RandomForestClassifier</code> to predict the target.</p>

<h2 id="dataframeschema---columntransformer-">DataFrameSchema -&gt; ColumnTransformer ü§Ø</h2>

<p>To define the <code class="language-plaintext highlighter-rouge">ColumnTransformer</code>, let‚Äôs take advantage of the fact that we‚Äôve
already declared a lot of the properties of our data with the <code class="language-plaintext highlighter-rouge">feature_schema</code>.
This is by no means a general solution, but the <code class="language-plaintext highlighter-rouge">column_transformer_from_schema</code>
function will do the job for this analysis. For categorical variables, we‚Äôre
going to pull out the discrete categories from the <code class="language-plaintext highlighter-rouge">str</code> columns by accessing
the <code class="language-plaintext highlighter-rouge">statistics</code> attribute of the <code class="language-plaintext highlighter-rouge">Check</code> object that defines the
<code class="language-plaintext highlighter-rouge">allowed_values</code>.</p>

<h2 id="define-the-transformer">Define the transformer</h2>

<p>And‚Ä¶ it works!</p>

<h2 id="define-and-fit-the-modeling-pipeline">Define and fit the modeling pipeline</h2>

<p>Okay, so let‚Äôs define the estimator and fit the model. But wait! We can use
the <code class="language-plaintext highlighter-rouge">check_input</code> decorator here to validate the feature and target inputs
that go into the <code class="language-plaintext highlighter-rouge">pipeline.fit</code> method. In this case, we can specify the
data to validate with a string that references the <code class="language-plaintext highlighter-rouge">X</code> and <code class="language-plaintext highlighter-rouge">y</code> argument names
of the sklearn <code class="language-plaintext highlighter-rouge">fit</code> API.</p>

<p>As I warned you earlier, I have a hammer, and everything‚Äôs a nail here.</p>

<h2 id="evaluate-the-model">Evaluate the Model</h2>

<p>We can apply the same pattern to model evaluation, where we can validate
the input and output of the <code class="language-plaintext highlighter-rouge">predict</code> method. Here we see we have a decent
model fit with a test set ROC AUC of 0.82 with some overfitting.</p>

<h3 id="plot-the-roc-curves-using-an-in-line-schema">Plot the ROC curves using an in-line schema</h3>

<p>And in this plotting code I want to show you that you can even define in-line
schemas that aren‚Äôt stored in a variable. This one just checks that the
false positive and true positive rates are values between 0 and 1.</p>

<h2 id="audit-the-model">Audit the Model</h2>

<p>To audit the model, we‚Äôre going to use the <code class="language-plaintext highlighter-rouge">shap</code> package, which implements
the Shapley Additive Explanations method for obtaining model explanations
I don‚Äôt have time to dig into the details of this method, but
at a high level <code class="language-plaintext highlighter-rouge">shap</code> values provide an estimate for a feature‚Äôs positive
or negative contribution to the model output per instance.</p>

<p>First we create an <code class="language-plaintext highlighter-rouge">explainer</code> for our random forest estimator. Then, we
use a decorated version of the pipeline <code class="language-plaintext highlighter-rouge">transformer</code> to obtain an array
of test set features to compute our <code class="language-plaintext highlighter-rouge">shap_values</code>.</p>

<h2 id="what-factors-are-most-predictive-of-the-court-ruling-a-case-as-accidental-1">What factors are most predictive of the court ruling a case as ‚ÄúAccidental‚Äù?</h2>

<p>Returning to our research question, the way to look at the graph is the
following: along the y-axis we have the features that go into predicting
the <code class="language-plaintext highlighter-rouge">accidental</code> case disposition. Each dot is a single training instance,
where red means that the value for that feature is high. Since most of the
features here are binary, red means that the value is 1, blue means 0.</p>

<p>Higher values on the x-axis means that the feature value contributed to a
higher probability of an <code class="language-plaintext highlighter-rouge">accidental</code> case disposition. The thing to look out
for is one particular color being predominantly one side of the zero x-axis
marker.</p>

<p>For example, the <code class="language-plaintext highlighter-rouge">cause_of_death</code> by <code class="language-plaintext highlighter-rouge">vehicle</code> contributes to a higher
probability that the predicted disposition is <code class="language-plaintext highlighter-rouge">accidental</code>, so does being
tasered, asphyxiated or restrained, or belonging to the <code class="language-plaintext highlighter-rouge">race_unspecified</code>
category.</p>

<p>Conversely, the <code class="language-plaintext highlighter-rouge">cause_of_death</code> of gunshot, race being white, or asian
pacific islander contributes to a lower probability that the predicted
disposition is <code class="language-plaintext highlighter-rouge">accidental</code>.</p>

<h2 id="write-the-model-audit-schema">Write the Model Audit Schema</h2>

<p>If we wanted to validate these findings, we can create a dataframe of feature
values and their associated <code class="language-plaintext highlighter-rouge">shap</code> value for each instance in the test set.</p>

<p>Then for each binary variable, we can define a two-sample ttest hypothesis test
asserting that having a value of <code class="language-plaintext highlighter-rouge">1</code> for a particular variable tends to have
a higher or lower set of shap values.</p>

<p>We can then programmatically construct a <code class="language-plaintext highlighter-rouge">model_audit_schema</code> that verifies
the properties that I outlined previously to see if the model explanations
are consistent with the hypotheses. We can see that, given this schema, our
model audit passes. We can use this same schema in future instantiations
of the model to see if the assumptions hold.</p>

<h2 id="more-questions-">More Questions ü§î</h2>

<p>Before I conclude the analysis, I just want to point out a few questions that
you may want to consider if you want to take a look at these data yourself.</p>

<p>For example, the plot below provides a hint at the last question, which looks
at interaction effects between the variable <code class="language-plaintext highlighter-rouge">race_african_american_black</code> and
<code class="language-plaintext highlighter-rouge">symptoms_of_mental_illness</code>. My interpretation of this plot is that,
‚Äúif the decedent was black, showing symptoms of mental illness contributed to 
a higher predicted probability of the <code class="language-plaintext highlighter-rouge">accidental</code> case disposition, whereas
being not black and showing symptoms of mental illness contributed to a lower
predicted probability‚Äù.</p>

<p>I‚Äôm sure there are a lot more questions that can be asked of this dataset, and
I‚Äôd like to invite you to take a look for yourself.</p>

<h2 id="takeaways">Takeaways</h2>

<p>In summary, there are four takeaways that I want to leave you with:</p>

<p>Data validation is a means to multiple ends: reproducibility, readability,
maintainability, and statistical type safety.</p>

<p>It‚Äôs an iterative process between exploring the data, acquiring domain
knowledge, and writing validation code.</p>

<p><code class="language-plaintext highlighter-rouge">pandera</code> schemas are executable contracts that enforce the statistical
properties of a dataframe at runtime and can be flexibly interleaved with data
processing and analysis logic.</p>

<p><code class="language-plaintext highlighter-rouge">pandera</code> doesn‚Äôt automate data exploration or the data validation process. The
user is responsible for identifying which parts of the pipeline are critical to
test and defining the contracts under which data are considered valid.</p>

<h2 id="experimental-features">Experimental Features</h2>

<p>If you want to try out this package, I‚Äôd be curious to learn about the utility
of some experimental features that come with version 0.4.0 and up. The first
is schema inference, which produces a schema using a dataframe input, and the
second is the ability to write a schema to a yaml file or a python script.</p>

<h2 id="roadmap-feature-proposals">Roadmap: Feature Proposals</h2>

<p>And in terms of features coming down the line, serving some pain points that
I have with training machine learning models with tabular data,
I thought it would be nice to have domain-specific schemas where you can
distinguish between target and feature variables, and then use that schema
to generate samples for testing model training code.</p>

<h2 id="contributions-are-welcome">Contributions are Welcome!</h2>

<p>If you‚Äôre interested in this project, there are many ways of contributing,
like improving documentation, submitting feature requests, bugs, and pull
requests!</p>

        </section>

        <footer>
        
          Pandera Presentations is maintained by <a href="https://github.com/pandera-dev">pandera-dev</a><br>
        
          This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.
        </footer>

      </div>
    </div>

    
  </body>
</html>
